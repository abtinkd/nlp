{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53dc760a",
   "metadata": {},
   "source": [
    "# Problem Statement: Hate Speech Classification\n",
    "\n",
    "Dataset using Twitter data, is was used to research hate-speech detection. The text is classified as: hate-speech, offensive language, and neither. Due to the nature of the study, it’s important to note that this dataset contains text that can be considered racist, sexist, homophobic, or generally offensive.\n",
    "\n",
    "## Column Description:\n",
    "- count:\n",
    "number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF)\n",
    "\n",
    "- hate_speech:\n",
    "number of CF users who judged the tweet to be hate speech\n",
    "\n",
    "- offensive_language:\n",
    "number of CF users who judged the tweet to be offensive\n",
    "\n",
    "- neither:\n",
    "number of CF users who judged the tweet to be neither offensive nor non-offensive\n",
    "\n",
    "- class:\n",
    "class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n",
    "\n",
    "- tweet:\n",
    "text tweet\n",
    "\n",
    "\n",
    "## Target Column:\n",
    "Need to predict the class column for test data by applying suitable NLP based algorithms.\n",
    "\n",
    "- class:\n",
    "class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16fa37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b33c0e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('speech_train.csv', dtype={'count':int, 'hate_speech':int, \n",
    "                                                       'offensive_language':int,'neither':int, \n",
    "                                                       'class':int, 'tweet':str})\n",
    "test_dataset = pd.read_excel('speech_test.xlsx', dtype={'class':float, 'tweet': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e36b61e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 499 entries, 0 to 498\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   count               499 non-null    int32 \n",
      " 1   hate_speech         499 non-null    int32 \n",
      " 2   offensive_language  499 non-null    int32 \n",
      " 3   neither             499 non-null    int32 \n",
      " 4   class               499 non-null    int32 \n",
      " 5   tweet               499 non-null    object\n",
      "dtypes: int32(5), object(1)\n",
      "memory usage: 13.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13ee9515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 164 entries, 0 to 163\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   class   0 non-null      float64\n",
      " 1   tweet   164 non-null    object \n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9cdbadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.763527\n",
       "2    0.168337\n",
       "0    0.068136\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['class'].value_counts() / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32098f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @HENNERGIZED: &amp;#128557;&amp;#128557;&amp;#128557;&amp;#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>We are back bitches! @vnpacheco21 @xoxoclaire_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @SuperrrrMcNasty: Lmfao , this bitch was gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@bradley_eckman don't say shit like that lil n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Young jezzy dat nigguh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            1                   2        0      1   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   3        0      1   \n",
       "4      3            1                   2        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  RT @HENNERGIZED: &#128557;&#128557;&#128557;&#...  \n",
       "1  We are back bitches! @vnpacheco21 @xoxoclaire_...  \n",
       "2  RT @SuperrrrMcNasty: Lmfao , this bitch was gi...  \n",
       "3  @bradley_eckman don't say shit like that lil n...  \n",
       "4                             Young jezzy dat nigguh  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb66abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @HENNERGIZED: &#128557;&#128557;&#128557;&#8220;@StopBeingSober: Yall say \"Why you dating lil girls\" like mature hoes just on a rampage outside.&#8221;',\n",
       " 'We are back bitches! @vnpacheco21 @xoxoclaire_ @treyhunter_ @Medgmon @delaney_jade @sean_steezy @alexistiefa',\n",
       " 'RT @SuperrrrMcNasty: Lmfao , this bitch was giving head in the back of a classroom she better go tf ahead !',\n",
       " \"@bradley_eckman don't say shit like that lil nig .. Or I'll give you a &#127812; stamp\",\n",
       " 'Young jezzy dat nigguh',\n",
       " 'Same shit RT @Che_TheYello1: But your bm tho? ... \"@viaNAWF: If my ex wanna go fuck my enemy, may God be with her. Ain\\'t my hoe no more.\"',\n",
       " 'RT @SumGurl07: So cute! :) RT @iTweetFacts: Shy bunny... http://t.co/z4u6NpORdz',\n",
       " 'This bitch always got shit to say about what other people like',\n",
       " \"RT @drewbillionaire: I just wanna pimp these hoe$ &amp; Ride Rarri's\",\n",
       " 'I heard them same pussy niggas hatin !',\n",
       " \"RT @michael4h2o: An #Ohio inmate sums up what #LeBronJames' return means for #Cleveland: http://t.co/a68mtZQOeL http://t.co/bMrzFWstVA #Leb&#8230;\",\n",
       " 'RT @OHHHME: I just want the money, ian trippin on these hoes',\n",
       " 'Y\\'all men need to watch these \"cream pies\" bitches be having yeast infections &amp; on a 7 day monistat treatment. Know the difference',\n",
       " 'RT @smithmuffins: once at a gradeschool dance i tried to do the worm to impress this bitch and i slammed my dick on the unforgiving ceramic&#8230;',\n",
       " 'Perhaps I\\'m not a \"diehard\" Yankee fan, but there\\'s no more point in watching this debacle #Yankees #Tigers #MLB So what else is on? LOL',\n",
       " 'The homie reallly LOVE fat bitches &#128514;&#128557;&#128514; like in LOVE',\n",
       " '@_Dbankss orr why? Thts my lor bae tho wyd why u fucking up hoe?',\n",
       " 'RT @Yankees: Time to bounce in the Bronx. #LetsGoYankees #Walkoff http://t.co/W5ATfRsLKK',\n",
       " 'Bitches rather be petty with his hoes instead of posting bail now that ah really fck em up.....psa',\n",
       " '\"@MaxMayo77: http://t.co/3Jk4kR44X3\" a pissed lad past out. I would lick his dirty soles while he slept.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tweet.head(20).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed74846d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @HENNERGIZED: 😭😭😭“@StopBeingSober: Yall say \"Why you dating lil girls\" like mature hoes just on a rampage outside.”',\n",
       " 'We are back bitches! @vnpacheco21 @xoxoclaire_ @treyhunter_ @Medgmon @delaney_jade @sean_steezy @alexistiefa',\n",
       " 'RT @SuperrrrMcNasty: Lmfao , this bitch was giving head in the back of a classroom she better go tf ahead !',\n",
       " \"@bradley_eckman don't say shit like that lil nig .. Or I'll give you a 🍄 stamp\",\n",
       " 'Young jezzy dat nigguh',\n",
       " 'Same shit RT @Che_TheYello1: But your bm tho? ... \"@viaNAWF: If my ex wanna go fuck my enemy, may God be with her. Ain\\'t my hoe no more.\"',\n",
       " 'RT @SumGurl07: So cute! :) RT @iTweetFacts: Shy bunny... http://t.co/z4u6NpORdz',\n",
       " 'This bitch always got shit to say about what other people like',\n",
       " \"RT @drewbillionaire: I just wanna pimp these hoe$ & Ride Rarri's\",\n",
       " 'I heard them same pussy niggas hatin !',\n",
       " \"RT @michael4h2o: An #Ohio inmate sums up what #LeBronJames' return means for #Cleveland: http://t.co/a68mtZQOeL http://t.co/bMrzFWstVA #Leb…\",\n",
       " 'RT @OHHHME: I just want the money, ian trippin on these hoes',\n",
       " 'Y\\'all men need to watch these \"cream pies\" bitches be having yeast infections & on a 7 day monistat treatment. Know the difference',\n",
       " 'RT @smithmuffins: once at a gradeschool dance i tried to do the worm to impress this bitch and i slammed my dick on the unforgiving ceramic…',\n",
       " 'Perhaps I\\'m not a \"diehard\" Yankee fan, but there\\'s no more point in watching this debacle #Yankees #Tigers #MLB So what else is on? LOL',\n",
       " 'The homie reallly LOVE fat bitches 😂😭😂 like in LOVE',\n",
       " '@_Dbankss orr why? Thts my lor bae tho wyd why u fucking up hoe?',\n",
       " 'RT @Yankees: Time to bounce in the Bronx. #LetsGoYankees #Walkoff http://t.co/W5ATfRsLKK',\n",
       " 'Bitches rather be petty with his hoes instead of posting bail now that ah really fck em up.....psa',\n",
       " '\"@MaxMayo77: http://t.co/3Jk4kR44X3\" a pissed lad past out. I would lick his dirty soles while he slept.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import html\n",
    "train_dataset['tweet'].apply(html.unescape).head(20).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6f4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge langdetect\n",
    "# !conda install -auto emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65853367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# detect language of the tweets\n",
    "from langdetect import detect\n",
    "train_dataset['lang'] = train_dataset.tweet.apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a70e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id</td>\n",
       "      <td>Young jezzy dat nigguh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>nl</td>\n",
       "      <td>@Channnteeel pussy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>af</td>\n",
       "      <td>Wish I got hoes like James Bond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>cs</td>\n",
       "      <td>Fuk dat bitch lol @dropolo voice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tl</td>\n",
       "      <td>Fucking gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>id</td>\n",
       "      <td>' Nah I ain't no hoe niggah , no bitch niggah ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>cy</td>\n",
       "      <td>I'll pay yall niggas to get lost, how much y'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>fi</td>\n",
       "      <td>Ronny kamm is a pussy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>cy</td>\n",
       "      <td>@GoHard_Brown @your_daddy9 &amp;amp; xavier you bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>af</td>\n",
       "      <td>Big night of &amp;#127936; #hoosiers #iubb #big10 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lang                                              tweet\n",
       "4     id                             Young jezzy dat nigguh\n",
       "21    nl                                 @Channnteeel pussy\n",
       "23    af                    Wish I got hoes like James Bond\n",
       "26    cs                   Fuk dat bitch lol @dropolo voice\n",
       "34    tl                                       Fucking gook\n",
       "59    id  ' Nah I ain't no hoe niggah , no bitch niggah ...\n",
       "88    cy  I'll pay yall niggas to get lost, how much y'a...\n",
       "106   fi                              Ronny kamm is a pussy\n",
       "145   cy  @GoHard_Brown @your_daddy9 &amp; xavier you bi...\n",
       "148   af  Big night of &#127936; #hoosiers #iubb #big10 ..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.loc[train_dataset['lang'] != 'en', ['lang', 'tweet']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2579ceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>lang</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>combined_class_votes</th>\n",
       "      <th>class2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @HENNERGIZED: &amp;#128557;&amp;#128557;&amp;#128557;&amp;#...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>We are back bitches! @vnpacheco21 @xoxoclaire_...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @SuperrrrMcNasty: Lmfao , this bitch was gi...</td>\n",
       "      <td>en</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            1                   2        0      1   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "\n",
       "                                               tweet lang         0         1  \\\n",
       "0  RT @HENNERGIZED: &#128557;&#128557;&#128557;&#...   en  0.333333  0.666667   \n",
       "1  We are back bitches! @vnpacheco21 @xoxoclaire_...   en  0.000000  1.000000   \n",
       "2  RT @SuperrrrMcNasty: Lmfao , this bitch was gi...   en  0.000000  1.000000   \n",
       "\n",
       "     2  combined_class_votes class2  \n",
       "0  0.0              0.666667    0.7  \n",
       "1  0.0              1.000000    1.0  \n",
       "2  0.0              1.000000    1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['0'] = train_dataset['hate_speech'] / train_dataset['count']\n",
    "train_dataset['1'] = train_dataset['offensive_language'] / train_dataset['count']\n",
    "train_dataset['2'] = train_dataset['neither'] / train_dataset['count']\n",
    "train_dataset['combined_class_votes'] = train_dataset['0'] * 0 + \\\n",
    "                                        train_dataset['1'] * 1 + \\\n",
    "                                        train_dataset['2'] * 2\n",
    "train_dataset['class2'] = np.round(train_dataset['combined_class_votes'], 1).astype(str)\n",
    "train_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df420b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.7', '1.0', '2.0', '0.8', '1.7', '0.3', '1.3', '1.8', '0.0',\n",
       "       '1.2'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.class2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42310898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loudly_crying_fac loudly_crying_fac loudly_crying_fac yall say date lil girl like matur hoe rampag outsid',\n",
       " 'back bitch',\n",
       " 'lmfao bitch give head back classroom better go tf ahead',\n",
       " 'say shit like lil nig give mushroom stamp',\n",
       " 'young jezzi dat nigguh',\n",
       " 'shit bm tho ex wanna go fuck enemi may god hoe',\n",
       " 'cute shi bunni',\n",
       " 'bitch alway got shit say peopl like',\n",
       " 'wanna pimp hoe ride rarri',\n",
       " 'heard pussi nigga hatin',\n",
       " 'ohio inmat sum lebronjam return mean cleveland leb',\n",
       " 'want money ian trippin hoe',\n",
       " 'men need watch cream pie bitch yeast infect day monistat treatment know differ',\n",
       " 'gradeschool danc tri worm impress bitch slam dick unforgiv ceram',\n",
       " 'perhap diehard yanke fan point watch debacl yanke tiger mlb els lol',\n",
       " 'homi reallli love fat bitch face_with_tears_of_joy loudly_crying_fac face_with_tears_of_joy like love',\n",
       " 'orr tht lor bae tho wyd u fuck hoe',\n",
       " 'time bounc bronx letsgoyanke walkoff',\n",
       " 'bitch rather petti hoe instead post bail ah realli fck em psa',\n",
       " 'piss lad past would lick dirti sole slept']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://codereview.stackexchange.com/questions/163446/cleaning-and-extracting-meaningful-text-from-tweets\n",
    "import re\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
    "import unicodedata\n",
    "import emoji\n",
    "import html\n",
    "\n",
    "twitter_stop_words = ['rt']\n",
    "stopword_set = set(stopwords.words(\"english\") + twitter_stop_words)\n",
    "# stemmer_func = WordNetLemmatizer().lemmatize\n",
    "stemmer_func = PorterStemmer().stem\n",
    "# stemmer_func = SnowballStemmer(language='english').stem\n",
    "# stemmer_func = lambda x: x\n",
    "\n",
    "def preprocess(raw_text, stop_words=stopword_set, stemmer_func=stemmer_func):\n",
    "    # normalize unicode text\n",
    "    normalized_text = unicodedata.normalize('NFKD', raw_text)\\\n",
    "        .encode('ascii', 'ignore')\\\n",
    "        .decode('utf-8', 'ignore')\n",
    "\n",
    "    # convert html entities to unicode\n",
    "    text = html.unescape(normalized_text)\n",
    "\n",
    "    # convert emojis to their corresponding text\n",
    "    emojiwords_text = emoji.demojize(text)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    link_free_text = re.sub(\"(https?:\\/\\/)(\\s)?(www\\.)?(\\s?)(\\w+\\.)*([\\w\\-\\s]+\\/)*([\\w-]+)\\/?\", \" \", emojiwords_text)\n",
    "\n",
    "    # remove user mentions\n",
    "    mention_free_text = re.sub(\"@([A-Za-z0-9_]+)\", \" \", link_free_text)\n",
    "\n",
    "    # remove unwanted characters\n",
    "    text = re.sub(\"[^a-zA-Z_]\", \" \", mention_free_text)\n",
    "\n",
    "    # convert to lower case and split \n",
    "    words = text.lower().split()\n",
    "\n",
    "    # remove stopwords\n",
    "    stemmed_words = [stemmer_func(w) for w in words if w not in stop_words]\n",
    "\n",
    "    # join the cleaned words in a list\n",
    "    cleaned_word_list = \" \".join(stemmed_words)\n",
    "\n",
    "    return cleaned_word_list\n",
    "\n",
    "train_dataset.tweet.apply(preprocess).head(20).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be5e7926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>class2</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>loudly_crying_fac loudly_crying_fac loudly_cry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>back bitch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>lmfao bitch give head back classroom better go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>say shit like lil nig give mushroom stamp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>young jezzi dat nigguh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class         0         1    2 class2  \\\n",
       "0      1  0.333333  0.666667  0.0    0.7   \n",
       "1      1  0.000000  1.000000  0.0    1.0   \n",
       "2      1  0.000000  1.000000  0.0    1.0   \n",
       "3      1  0.000000  1.000000  0.0    1.0   \n",
       "4      1  0.333333  0.666667  0.0    0.7   \n",
       "\n",
       "                                               tweet  \n",
       "0  loudly_crying_fac loudly_crying_fac loudly_cry...  \n",
       "1                                         back bitch  \n",
       "2  lmfao bitch give head back classroom better go...  \n",
       "3          say shit like lil nig give mushroom stamp  \n",
       "4                             young jezzi dat nigguh  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_dataset = train_dataset.drop(axis=1, columns=\n",
    "['hate_speech', 'offensive_language', 'neither', 'count', 'tweet', 'combined_class_votes', 'lang'])\n",
    "clean_test_dataset = test_dataset.drop(axis=1, columns=['tweet'])\n",
    "clean_train_dataset['tweet'] = train_dataset['tweet'].apply(preprocess)\n",
    "clean_test_dataset['tweet'] = test_dataset['tweet'].apply(preprocess)\n",
    "clean_train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507300f0",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80af8b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.0' '1.0' '1.0' '1.0' '1.0' '2.0' '1.0' '1.0' '1.0' '1.0' '0.7' '1.0'\n",
      " '1.0' '1.0' '2.0']\n",
      "0.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "corpus = clean_train_dataset['tweet'].to_list() + clean_test_dataset['tweet'].to_list()\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "X = vectorizer.transform(clean_train_dataset['tweet'])\n",
    "y = clean_train_dataset[['class', 'class2']]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# model = LogisticRegression(multi_class='multinomial', max_iter=700)\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "predict_use_more_classes = True\n",
    "\n",
    "if not predict_use_more_classes:\n",
    "    model.fit(X_train, y_train['class'])\n",
    "    y_pred = model.predict(X_valid)\n",
    "    print(accuracy_score(y_pred, y_valid['class']))\n",
    "else:\n",
    "    model.fit(X_train, y_train['class2'])\n",
    "    y_pred = model.predict(X_valid)\n",
    "    print(y_pred[:15])\n",
    "    y_pred = np.round(y_pred.astype(float), 0).astype(int)\n",
    "    print(accuracy_score(y_pred, y_valid['class']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afab944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(clean_test_dataset['tweet'])\n",
    "if not predict_use_more_classes:\n",
    "    clean_test_dataset['class'] = model.predict(X_test)\n",
    "else:\n",
    "    predictions = model.predict(X_test)\n",
    "    clean_test_dataset['class'] = np.round(predictions.astype(float), 0).astype(int)\n",
    "    \n",
    "clean_test_dataset.to_csv('speech_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a3925",
   "metadata": {},
   "source": [
    "# LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad3c5d4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abtin\\.conda\\envs\\nlp\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "def get_dictionary(train_docs, test_docs):\n",
    "    dictionary = gensim.corpora.Dictionary(train_docs)\n",
    "    dictionary.merge_with(gensim.corpora.Dictionary(test_docs))\n",
    "    return dictionary\n",
    "\n",
    "train_docs = clean_train_dataset['tweet'].apply(str.split)\n",
    "test_docs = clean_test_dataset['tweet'].apply(str.split)\n",
    "dictionary = get_dictionary(train_docs, test_docs)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in train_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de00eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 3, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 100,\n",
    "                                   workers = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c9c2804dcc4687100d54c573fede56048ce925a772e691277d931edc426c058"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
